--- linux-5.4.41/include/linux/dma-mapping.h	2020-05-14 01:58:30.000000000 -0400
+++ linux/include/linux/dma-mapping.h	2020-05-17 00:44:04.297761937 -0400
@@ -263,9 +263,9 @@
 
 static inline const struct dma_map_ops *get_dma_ops(struct device *dev)
 {
-	if (dev->dma_ops)
+	if (dev && dev->dma_ops)
 		return dev->dma_ops;
-	return get_arch_dma_ops(dev->bus);
+	return get_arch_dma_ops(dev ? dev->bus : NULL);
 }
 
 static inline void set_dma_ops(struct device *dev,
@@ -658,7 +658,7 @@
 
 static inline u64 dma_get_mask(struct device *dev)
 {
-	if (dev->dma_mask && *dev->dma_mask)
+	if (dev && dev->dma_mask && *dev->dma_mask)
 		return *dev->dma_mask;
 	return DMA_BIT_MASK(32);
 }
--- linux-5.4.41/kernel/dma/direct.c	2020-05-14 01:58:30.000000000 -0400
+++ linux/kernel/dma/direct.c	2020-05-17 00:45:42.214310531 -0400
@@ -327,7 +327,7 @@
 		size_t size)
 {
 	return swiotlb_force != SWIOTLB_FORCE &&
-		dma_capable(dev, dma_addr, size);
+		(!dev || dma_capable(dev, dma_addr, size));
 }
 
 dma_addr_t dma_direct_map_page(struct device *dev, struct page *page,
--- linux-5.4.41/kernel/dma/mapping.c	2020-05-14 01:58:30.000000000 -0400
+++ linux/kernel/dma/mapping.c	2020-05-17 00:47:34.830945774 -0400
@@ -300,7 +300,7 @@
 	const struct dma_map_ops *ops = get_dma_ops(dev);
 	void *cpu_addr;
 
-	WARN_ON_ONCE(!dev->coherent_dma_mask);
+	WARN_ON_ONCE(dev && !dev->coherent_dma_mask);
 
 	if (dma_alloc_from_dev_coherent(dev, size, dma_handle, &cpu_addr))
 		return cpu_addr;
